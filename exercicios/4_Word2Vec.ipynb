{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zljehcW10P5q"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/alan-barzilay/NLPortugues/master/imagens/logo_nlportugues.png\"   width=\"150\" align=\"right\">\n",
        "\n",
        "# Lista 4 - Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bUareHn6UqM"
      },
      "source": [
        "______________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw8FPTMX0P5s"
      },
      "source": [
        "Nessa lista nós exploraremos o espaço vetorial gerado pelo algoritmo Word2Vec e algumas de suas propriedades mais interessantes. Veremos como palavras similares se organizam nesse espaço e as relações de palavras com seus sinônimos e antônimos. Também veremos algumas analogias interessantes que o algoritmo é capaz de fazer ao capturar um pouco do nosso uso da língua portuguesa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vR52KVnq0P5t"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KUL7X3F0P5u"
      },
      "source": [
        "# Carregando dados\n",
        "\n",
        "\n",
        "Para esta lista nós utilizaremos vetores de palavras, também conhecidos como *embeddings*, para lingua portuguesa fornecidos pelo [NILC](http://www.nilc.icmc.usp.br/nilc/index.php). Nós utilizaremos o embedding de 50 dimensões treinado com o algoritmo Word2Vec (Continous Bag of Words) que pode ser encontrado [aqui](http://www.nilc.icmc.usp.br/embeddings) sob a licença [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Para evitar problemas de mémoria utilizaremos apenas as 200 mil palavras mais comum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sEwqxBvD0Rga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bbb32949-efd6-4482-b33e-da5d4d960ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 92.0M  100 92.0M    0     0  1374k      0  0:01:08  0:01:08 --:--:-- 21.5M\n"
          ]
        }
      ],
      "source": [
        "!curl  https://raw.githubusercontent.com/alan-barzilay/NLPortugues/master/Semana%2004/data/word2vec_200k.txt --output 'word2vec_200k.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bwajr5sQ0P5v"
      },
      "outputs": [],
      "source": [
        "# Carrega word2vec\n",
        "model = KeyedVectors.load_word2vec_format(\"word2vec_200k.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2JYtS1k0P5v"
      },
      "source": [
        "# Similaridade e Distância Cosseno\n",
        "\n",
        "Como comentamos em sala de aula, podemos considerar as palavras como pontos num espaço n-dimensional e podemos examinar a proximidade delas através da similaridade cosseno:\n",
        "$$s = \\frac{u \\cdot v}{||u|| ||v||}, \\textrm{ onde } s \\in [-1, 1] $$\n",
        "\n",
        "\n",
        "## <font color='blue'>Questão 1 </font>\n",
        "Palavras [polissemicas](https://pt.wikipedia.org/wiki/Polissemia) e [homônimas](https://pt.wikipedia.org/wiki/Hom%C3%B3nimo) são palavras que possuem mais de um significado.\n",
        "\n",
        "\n",
        "Utilizando a função `model.most_similar()`, encontre uma palavra que possua múltiplos significados e que na sua lista de 10 palavras mais similares existam palavras relacionadas a mais de um dos seus significados, lembre-se de consultar sua [documentação](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar).\n",
        "\n",
        "Por exemplo, a palavra \"manga\" possui na sua lista de 10 palavras mais similares as palavras \"gola\" e \"lapela\" (que estão relacionadas ao significado de manga de uma camiseta) e a palavra \"maçã\" (que está relacionada ao significado da fruta manga).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 575,
      "metadata": {
        "id": "qbvAUIa30P5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f722003-7eac-4ef5-c29a-9770a7b38811"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cantar', 0.909256637096405),\n",
              " ('dançar', 0.8524582386016846),\n",
              " ('filmar', 0.8430111408233643),\n",
              " ('namorar', 0.8334317803382874),\n",
              " ('fotografar', 0.8212646842002869),\n",
              " ('aparecer', 0.8181331753730774),\n",
              " ('experimentar', 0.8166038990020752),\n",
              " ('brilhar', 0.8126950860023499),\n",
              " ('ensaiar', 0.8113200664520264),\n",
              " ('trabalhar', 0.810289204120636)]"
            ]
          },
          "metadata": {},
          "execution_count": 575
        }
      ],
      "source": [
        "# Seu código aqui\n",
        "model.most_similar('tocar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLh82IfO0P5x"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "\n",
        "\"tocar\" - significados:\n",
        "\n",
        "1. cantar, dançar, filmar, ensaiar -> relacionado a \"tocar um instrumento\"\n",
        "2. namorar, experimentar -> relacionado a \"encostar\", \"por a mão\"\n",
        "3. trabalhar -> talvez relacionado ao sentido de \"tocar um negócio/empresa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6YKYZ_z0P5x"
      },
      "source": [
        "# Sinônimos e Antônimos\n",
        "\n",
        "\n",
        "As vezes é mais intuitivo trabalhar com uma medida de distancia ao invés da similaridade cosseno, para isso vamos utilizar a distancia cosseno que é simplesmente 1 - Similaridade Cosseno.\n",
        "\n",
        "## <font color='blue'>Questão 2 </font>\n",
        "\n",
        "\n",
        "Usando a função [`model.distance(palavra1,palavra2)`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance), encontre 3 palavras onde as palavras p1 e p2 são sinônimas e p1 e p3 são antônimas mas `distance(p1,p3)` < `distance(p1,p2)`.\n",
        "\n",
        "Proponha uma explicação do porque esse resultado contraintuitivo acontece.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 574,
      "metadata": {
        "id": "JeywFdKk0P5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7fddad2d-f233-40c1-d539-3c1a8c331169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p1 = \"iniciar\"\n",
            "p2 = \"começar\"\n",
            "p3 = \"encerrar\"\n",
            "\n",
            "sinônimos (p1, p2):  0.2515414357185364\n",
            "antônimos (p1, p3):  0.10052013397216797\n"
          ]
        }
      ],
      "source": [
        "# Seu código aqui\n",
        "print('p1 = \"iniciar\"')\n",
        "print('p2 = \"começar\"')\n",
        "print('p3 = \"encerrar\"')\n",
        "\n",
        "print()\n",
        "print(\"sinônimos (p1, p2): \", model.distance(\"iniciar\", \"começar\"))\n",
        "print(\"antônimos (p1, p3): \", model.distance(\"iniciar\", \"encerrar\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ncOIj240P5y"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "Como os embeddings são montados de acordo com a ocorrência das palavras nos textos do córpus, é provável que as palavras \"iniciar\" e \"encerrar\" apareçam em contextos mais similares do que ocorre com as palavras \"iniciar\" e \"começar\", assim os vetores que representam os ântônimos acabam ficando mais similares do que os vetores que representam os sinônimos neste caso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Se982Q0P5z"
      },
      "source": [
        "# Analogias\n",
        "\n",
        "Existem algumas analogias famosas realizadas por vetores de palavras. O exemplo mais famoso é provavelmente \"man : king :: woman : x\", onde x é *queen*.\n",
        "\n",
        "Para formular analogias vamos utilizar a função `most_similar()` que busca as palavras mais similares as listas em  `positive` e mais dissimilares as listadas em  `negative`. Para mais detalhes recomendamos consultar a sua [documentação](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "A8zujhY70P5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "421460ad-46e6-4bac-f0af-68676843e7f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('engenheira', 0.7883446216583252),\n",
              " ('investigadora', 0.7415961623191833),\n",
              " ('ex-funcionária', 0.7373332977294922),\n",
              " ('enfermeira', 0.7346670627593994),\n",
              " ('funcionária', 0.7172971367835999),\n",
              " ('bibliotecária', 0.7110162377357483),\n",
              " ('arquiteta', 0.7099220156669617),\n",
              " ('empresária', 0.7055575847625732),\n",
              " ('ex-diretora', 0.7055395841598511),\n",
              " ('professora', 0.697813868522644)]"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ],
      "source": [
        "model.most_similar(positive=['mulher', 'engenheiro'], negative=['homem']) # homem:engenheiro :: mulher:x -> x = engenheira"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJnuDjo-0P5z"
      },
      "source": [
        "## <font color='blue'>Questão 3 </font>\n",
        "Encontre analogias que funcionam, ou seja, que a palavra esperada está no topo da lista.\n",
        "\n",
        "Descreva sua analogia na seguinte forma:\n",
        "x:y :: a:b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "BXpu7g3a0P50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8515091e-5018-4104-a1ed-cd3773cc46e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('amiga', 0.8684101104736328)\n",
            "('professora', 0.7869903445243835)\n",
            "('herbívoro', 0.6485663652420044)\n",
            "('gremista', 0.9042074680328369)\n"
          ]
        }
      ],
      "source": [
        "# Seu código aqui\n",
        "print(model.most_similar(positive=['mulher', 'amigo'], negative=['homem'])[0])\n",
        "print(model.most_similar(positive=['mulher', 'professor'], negative=['homem'])[0])\n",
        "print(model.most_similar(positive=['vegetal', 'carnívoro'], negative=['carne'])[0])\n",
        "print(model.most_similar(positive=['grêmio', 'palmeirense'], negative=['palmeiras'])[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788u4d3A0P50"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "---\n",
        "\n",
        "homem:amigo :: mulher:x -> amiga\n",
        "\n",
        "homem:professor :: mulher:x -> professora\n",
        "\n",
        "carne:carnívoro :: vegetal:x -> herbívoro\n",
        "\n",
        "palmeirense:palmeiras :: gremista:x -> grêmio\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su8svdBl0P50"
      },
      "source": [
        "## <font color='blue'>Questão 4 </font>\n",
        "Encontre analogias que **Não** funcionam.\n",
        "\n",
        "Descreva o resultado esperado da sua analogia na seguinte forma:\n",
        "x:y :: a:b\n",
        "\n",
        "E indique o valor errado de b encontrado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {
        "id": "PdQ2rtyA0P51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "295ae8e1-c2c6-4d9c-a079-dc82eafa3f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('funcionária', 0.7913923859596252)\n",
            "('diarista', 0.683979868888855)\n",
            "('gremista', 0.9147422909736633)\n"
          ]
        }
      ],
      "source": [
        "# Seu código aqui\n",
        "print(model.most_similar(positive=['mulher', 'médico'], negative=['homem'])[0])\n",
        "print(model.most_similar(positive=['mulher', 'doutor'], negative=['homem'])[0])\n",
        "print(model.most_similar(positive=['corinthians', 'palmeirense'], negative=['palmeiras'])[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CMzT3fy0P51"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "---\n",
        "\n",
        "homem:médico :: mulher:x -> esperado \"médica\", mas retornou \"funcionária\"\n",
        "\n",
        "homem:médico :: mulher:x -> esperado \"doutora\", mas retornou \"diarista\"\n",
        "\n",
        "palmeirense:palmeiras :: corinthians:x -> esperado \"coritiano\", mas retornou \"gremista\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8LYWJ1i0P51"
      },
      "source": [
        "# Viés e preconceito adquirido\n",
        "\n",
        "Como estes vetores são aprendidos a partir de documentos produzidos dentro do nosso contexto social, eles podem capturar viéses, preconceitos e desigualdades presentes na nossa sociedade.\n",
        "\n",
        "Quando esses tipos de modelos começaram a ser treinados, cientistas ficaram surpresos com o fato de que tais viéses fossem aprendidos pelos modelos, mesmo quando os dados não continham textos específicos que defendiam posições preconceituosas ou que usassem linguagem explicitamente preconceituosa. Modelos treinados com textos a priori neutros e bastante revisados, ainda apresentam tais viéses, porque usualmente os viéses observados são fruto de preconceitos estruturais que perpassam toda a produção de uma sociedade, inclusive sua emissão linguística. Esses viéses, muitas vezes, não são objetivamente detectáveis em nível de texto individual, mas se manifestam nas correlações das palavras de forma transversal à qualquer grande volume de textos.\n",
        "\n",
        "É importante estar ciente desse tipo de viés dos modelos treinados e dos seus perigos. Aplicações que utilizam esses modelos podem perpetuar e até exarcebar desigualdades sociais, retro-alimentando as estruturas que os originaram.\n",
        "\n",
        "Para um exemplo mais concreto, vide a seguinte analogia preconceituosa capturada:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {
        "id": "53KYiqsc0P51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d1e05599-8af6-45c3-d8a4-661b5529b6ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('branco', 0.663209080696106),\n",
              " ('alegre/rs', 0.6620162725448608),\n",
              " ('braga-fc', 0.6464027762413025),\n",
              " ('sporting-fc', 0.6254758238792419),\n",
              " ('côvo', 0.6254613995552063),\n",
              " ('alegre-rs', 0.6199708580970764),\n",
              " ('vermelho', 0.612277090549469),\n",
              " ('covo', 0.604120671749115),\n",
              " ('cirílicos', 0.6022458672523499),\n",
              " ('benfica-fc', 0.5965930819511414)]"
            ]
          },
          "metadata": {},
          "execution_count": 511
        }
      ],
      "source": [
        "model.most_similar(positive=['negro', 'rico'], negative=['pobre'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS-sruEp0P52"
      },
      "source": [
        "Note também como diferem as palavras mais semelhantes a homem e mulher:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "bgtl4cgN0P53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8a589304-6149-4b6e-f11c-e6e75070efc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('monstro', 0.9085395932197571),\n",
              " ('bebé', 0.9072304368019104),\n",
              " ('indivíduo', 0.9050756096839905),\n",
              " ('rapaz', 0.9036115407943726),\n",
              " ('mendigo', 0.9007540345191956),\n",
              " ('rapazola', 0.8992964029312134),\n",
              " ('novelo', 0.8938027620315552),\n",
              " ('pássaro', 0.8897998929023743),\n",
              " ('cão', 0.8882535099983215),\n",
              " ('cãozinho', 0.8869855403900146)]"
            ]
          },
          "metadata": {},
          "execution_count": 363
        }
      ],
      "source": [
        "model.most_similar(\"homem\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "bExfFYGS0P53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dbbe8e67-daf9-4280-ade3-c28304fa5279"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('menina', 0.911119282245636),\n",
              " ('amiga', 0.9089193344116211),\n",
              " ('cadela', 0.9035040140151978),\n",
              " ('rapariga', 0.899989902973175),\n",
              " ('enfermeira', 0.8974366784095764),\n",
              " ('namorada', 0.8954240083694458),\n",
              " ('cafetina', 0.8932163119316101),\n",
              " ('prostituta', 0.8917951583862305),\n",
              " ('garota', 0.8906298279762268),\n",
              " ('cadelinha', 0.8902611136436462)]"
            ]
          },
          "metadata": {},
          "execution_count": 364
        }
      ],
      "source": [
        "model.most_similar(\"mulher\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgNf_9-P0P53"
      },
      "source": [
        "## <font color='blue'>Questão 5 </font>\n",
        "\n",
        "Utiliza a função `most_similar()` para encontrar um outro caso de viés adquirido pelos vetores e explique brevemente o tipo de viés encontrado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 570,
      "metadata": {
        "id": "FuHqTKSB0P53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6e5b3128-f7ba-44fe-b97b-029dd1186ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('confidente', 0.8713893294334412), ('ex-amante', 0.8611692786216736), ('sobrinha', 0.8388965129852295), ('neta', 0.8151741027832031), ('filha', 0.810907781124115), ('serviçal', 0.8079167604446411), ('viúva', 0.8032019138336182), ('sobrinha-neta', 0.7977979779243469), ('discípula', 0.7934637665748596), ('meia-irmã', 0.7928621768951416)]\n",
            "[('faca', 0.8725667595863342), ('lamparina', 0.8645756244659424), ('espingarda', 0.8556493520736694), ('arma', 0.850627601146698), ('navalha', 0.8492265939712524), ('bomba', 0.8482476472854614), ('baioneta', 0.8426659107208252), ('pancada', 0.8423606157302856), ('lâmina', 0.8406084179878235), ('corda', 0.8389527201652527)]\n"
          ]
        }
      ],
      "source": [
        "# Seu código aqui\n",
        "print(model.most_similar('amante'))\n",
        "print(model.most_similar('bala'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TID_Rbk70P53"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "A palavra \"amante\", embora não tenha gênero definido, está muito mais associada ao gênero feminino (\"sobrinha\", \"neta\", \"filha\", \"viúva\", \"meia-irmã\") - o que demonstra um viés sexista da base de dados.\n",
        "\n",
        "No segundo exemplo, a palavra \"bala\" está muito mais associada a munição de armas do que ao doce. Embora não seja exatamente um viés da sociedade ou preconceito, demonstra que há um viés no córpus utilizado para gerar os embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ih_CvL0P53"
      },
      "source": [
        "## <font color='blue'>Questão 6 </font>\n",
        "\n",
        "Qual é a possivel origem desses vieses? Tente explicar como eles podem ter sido capturados pelos vetores de palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm8ty0WH0P54"
      },
      "source": [
        "\n",
        "**<font color='red'> Sua resposta aqui </font>**\n",
        "\n",
        "Provavelmente os textos com a palavra \"amante\" estão associando a palavra ao gênero feminino, por exemplo: \"Ele tem uma amante\", \"Fulana é a amante dele\".\n",
        "\n",
        "Semelhantemente, no segundo caso, é provável que a grande maioria dos textos contendo a palavra \"bala\", esteja se referindo a armas."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv_tutorial",
      "language": "python",
      "name": ".venv_tutorial"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}